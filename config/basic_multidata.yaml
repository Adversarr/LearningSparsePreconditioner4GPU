defaults:
  - gnn
  - data
  - trainer # for dataset splitting and batch_size
  - loss
  - _self_

exp_name: ???
batch_size: 4
data:
  prefix: generated/${exp_name}
  all_prefix:
    - generated/elast_twist_r8
    - generated/elast_twist_r10
    - generated/elast_twist_r12
    - generated/elast_twist_r14
    # We wish to generalize to a finer mesh.
    # - generated/elast_twist_r16

    - generated/twist-tiny-box-remesh-1e-3
    - generated/twist-tiny-box-remesh-1e-4
    - generated/twist-tiny-box-remesh-3e-3
    - generated/twist-tiny-box-remesh-3e-4
    # We wish to generalize to a finer mesh.
    # - generated/twist-tiny-box-remesh-6e-5

epsilon: 3.0e-3 # Cannot be too small.
check_converge: true
pretrained: ""
seed: 42

check_methods:
  - none
  - ic
  - ainv
  - fsai
  - diagonal
check_devices:
  - cpu
  # # Note: in most cases, directly measure cuda time while training is meaning less.
  # - cuda
